<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8796f41f566a0a8ebb72863a83d558ed",
  "translation_date": "2025-08-24T21:19:53+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "fa"
}
-->
# مقدمه‌ای بر اخلاق داده‌ها

|![طرح مفهومی توسط [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| اخلاق در علم داده - _طرح مفهومی توسط [@nitya](https://twitter.com/nitya)_ |

---

ما همه شهروندان دنیای داده‌محور هستیم.

روندهای بازار نشان می‌دهند که تا سال ۲۰۲۲، یک‌سوم از سازمان‌های بزرگ داده‌های خود را از طریق [بازارها و مبادلات آنلاین](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) خرید و فروش خواهند کرد. به عنوان **توسعه‌دهندگان اپلیکیشن**، ما راحت‌تر و ارزان‌تر می‌توانیم بینش‌های مبتنی بر داده و اتوماسیون مبتنی بر الگوریتم را در تجربه‌های روزمره کاربران ادغام کنیم. اما با گسترش هوش مصنوعی، باید به آسیب‌های احتمالی ناشی از [سوءاستفاده](https://www.youtube.com/watch?v=TQHs8SA1qpk) از این الگوریتم‌ها در مقیاس بزرگ نیز توجه کنیم.

روندها همچنین نشان می‌دهند که تا سال ۲۰۲۵ بیش از [۱۸۰ زتابایت](https://www.statista.com/statistics/871513/worldwide-data-created/) داده تولید و مصرف خواهیم کرد. به عنوان **دانشمندان داده**، این به ما دسترسی بی‌سابقه‌ای به داده‌های شخصی می‌دهد. این بدان معناست که می‌توانیم پروفایل‌های رفتاری کاربران را بسازیم و تصمیم‌گیری‌ها را به گونه‌ای تحت تأثیر قرار دهیم که یک [توهم انتخاب آزاد](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ایجاد کند، در حالی که ممکن است کاربران را به سمت نتایج دلخواه ما سوق دهد. این موضوع همچنین سوالات گسترده‌تری در مورد حریم خصوصی داده‌ها و حفاظت از کاربران مطرح می‌کند.

اخلاق داده‌ها اکنون به عنوان _محافظ‌های ضروری_ برای علم داده و مهندسی عمل می‌کنند و به ما کمک می‌کنند تا آسیب‌های احتمالی و پیامدهای ناخواسته ناشی از اقدامات مبتنی بر داده‌های خود را به حداقل برسانیم. [چرخه هیجان گارتنر برای هوش مصنوعی](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) روندهای مرتبط با اخلاق دیجیتال، هوش مصنوعی مسئولانه و حاکمیت هوش مصنوعی را به عنوان عوامل کلیدی برای روندهای بزرگ‌تر در زمینه _دموکراتیزه‌سازی_ و _صنعتی‌سازی_ هوش مصنوعی شناسایی می‌کند.

![چرخه هیجان گارتنر برای هوش مصنوعی - ۲۰۲۰](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

در این درس، ما به بررسی حوزه جذاب اخلاق داده‌ها خواهیم پرداخت - از مفاهیم و چالش‌های اصلی گرفته تا مطالعات موردی و مفاهیم کاربردی هوش مصنوعی مانند حاکمیت - که به ایجاد فرهنگ اخلاقی در تیم‌ها و سازمان‌هایی که با داده‌ها و هوش مصنوعی کار می‌کنند، کمک می‌کند.

## [آزمون پیش از درس](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/2) 🎯

## تعاریف پایه

بیایید با درک اصطلاحات پایه شروع کنیم.

کلمه "اخلاق" از [کلمه یونانی "ethikos"](https://en.wikipedia.org/wiki/Ethics) (و ریشه آن "ethos") به معنای _شخصیت یا طبیعت اخلاقی_ گرفته شده است.

**اخلاق** درباره ارزش‌های مشترک و اصول اخلاقی است که رفتار ما را در جامعه هدایت می‌کنند. اخلاق بر اساس قوانین نیست، بلکه بر اساس هنجارهای پذیرفته‌شده عمومی از آنچه "درست در مقابل نادرست" است، استوار است. با این حال، ملاحظات اخلاقی می‌توانند بر ابتکارات حاکمیت شرکتی و مقررات دولتی تأثیر بگذارند که انگیزه‌های بیشتری برای رعایت قوانین ایجاد می‌کنند.

**اخلاق داده‌ها** یک [شاخه جدید از اخلاق](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) است که "مشکلات اخلاقی مرتبط با _داده‌ها، الگوریتم‌ها و شیوه‌های مربوطه_ را مطالعه و ارزیابی می‌کند". در اینجا، **"داده‌ها"** بر اقدامات مربوط به تولید، ثبت، مدیریت، پردازش، انتشار، اشتراک‌گذاری و استفاده تمرکز دارد، **"الگوریتم‌ها"** بر هوش مصنوعی، عوامل، یادگیری ماشین و ربات‌ها تمرکز دارد، و **"شیوه‌ها"** بر موضوعاتی مانند نوآوری مسئولانه، برنامه‌نویسی، هک و کدهای اخلاقی تمرکز دارد.

**اخلاق کاربردی** [کاربرد عملی ملاحظات اخلاقی](https://en.wikipedia.org/wiki/Applied_ethics) است. این فرآیند بررسی فعال مسائل اخلاقی در زمینه _اقدامات، محصولات و فرآیندهای دنیای واقعی_ و اتخاذ اقدامات اصلاحی برای اطمینان از هم‌راستایی آن‌ها با ارزش‌های اخلاقی تعریف‌شده ما است.

**فرهنگ اخلاقی** درباره [_عملیاتی کردن_ اخلاق کاربردی](https://hbr.org/2019/05/how-to-design-an-ethical-organization) است تا اطمینان حاصل شود که اصول و شیوه‌های اخلاقی ما به طور مداوم و در مقیاس وسیع در سراسر سازمان پذیرفته می‌شوند. فرهنگ‌های اخلاقی موفق اصول اخلاقی سازمانی را تعریف می‌کنند، انگیزه‌های معناداری برای رعایت قوانین ارائه می‌دهند و هنجارهای اخلاقی را با تشویق و تقویت رفتارهای مطلوب در هر سطح از سازمان تقویت می‌کنند.

## مفاهیم اخلاقی

در این بخش، مفاهیمی مانند **ارزش‌های مشترک** (اصول) و **چالش‌های اخلاقی** (مشکلات) برای اخلاق داده‌ها را بررسی خواهیم کرد - و به **مطالعات موردی** خواهیم پرداخت که به شما کمک می‌کنند این مفاهیم را در زمینه‌های دنیای واقعی درک کنید.

### 1. اصول اخلاقی

هر استراتژی اخلاق داده‌ها با تعریف _اصول اخلاقی_ آغاز می‌شود - "ارزش‌های مشترکی" که رفتارهای قابل قبول را توصیف می‌کنند و اقدامات مطابق با قوانین را در پروژه‌های داده و هوش مصنوعی ما هدایت می‌کنند. شما می‌توانید این اصول را در سطح فردی یا تیمی تعریف کنید. با این حال، بیشتر سازمان‌های بزرگ این اصول را در قالب یک بیانیه مأموریت یا چارچوب اخلاقی هوش مصنوعی تعریف می‌کنند که در سطح شرکتی تعریف شده و به طور مداوم در تمام تیم‌ها اجرا می‌شود.

**مثال:** بیانیه مأموریت [هوش مصنوعی مسئولانه](https://www.microsoft.com/en-us/ai/responsible-ai) مایکروسافت می‌گوید: _"ما متعهد به پیشرفت هوش مصنوعی بر اساس اصول اخلاقی هستیم که انسان‌ها را در اولویت قرار می‌دهد"_ - و ۶ اصل اخلاقی را در چارچوب زیر شناسایی می‌کند:

![هوش مصنوعی مسئولانه در مایکروسافت](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

بیایید این اصول را به طور خلاصه بررسی کنیم. _شفافیت_ و _پاسخگویی_ ارزش‌های بنیادی هستند که سایر اصول بر اساس آن‌ها ساخته شده‌اند - بنابراین از اینجا شروع می‌کنیم:

* [**پاسخگویی**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) متخصصان را _مسئول_ عملیات داده و هوش مصنوعی خود و رعایت این اصول اخلاقی می‌کند.
* [**شفافیت**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) اطمینان می‌دهد که اقدامات داده و هوش مصنوعی برای کاربران _قابل درک_ هستند و دلیل تصمیمات را توضیح می‌دهند.
* [**عدالت**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - بر اطمینان از رفتار عادلانه هوش مصنوعی با _همه افراد_ تمرکز دارد و به تعصبات سیستماتیک یا ضمنی در داده‌ها و سیستم‌ها می‌پردازد.
* [**قابلیت اطمینان و ایمنی**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - اطمینان می‌دهد که هوش مصنوعی به طور _ثابت_ با ارزش‌های تعریف‌شده رفتار می‌کند و آسیب‌های احتمالی یا پیامدهای ناخواسته را به حداقل می‌رساند.
* [**حریم خصوصی و امنیت**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - درباره درک منشأ داده‌ها و ارائه _حفاظت‌های مرتبط با حریم خصوصی داده‌ها_ به کاربران است.
* [**شمول‌گرایی**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - درباره طراحی راه‌حل‌های هوش مصنوعی با نیت و تطبیق آن‌ها برای پاسخگویی به _نیازها و قابلیت‌های متنوع انسانی_ است.

> 🚨 به این فکر کنید که بیانیه مأموریت اخلاق داده‌های شما چه می‌تواند باشد. چارچوب‌های اخلاقی هوش مصنوعی از سازمان‌های دیگر را بررسی کنید - اینجا مثال‌هایی از [IBM](https://www.ibm.com/cloud/learn/ai-ethics)، [گوگل](https://ai.google/principles)، و [فیسبوک](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/) آورده شده است. چه ارزش‌های مشترکی در آن‌ها وجود دارد؟ این اصول چگونه به محصول هوش مصنوعی یا صنعتی که در آن فعالیت می‌کنند مرتبط هستند؟

### 2. چالش‌های اخلاقی

پس از تعریف اصول اخلاقی، گام بعدی ارزیابی اقدامات داده و هوش مصنوعی برای بررسی هم‌راستایی آن‌ها با این ارزش‌های مشترک است. اقدامات خود را در دو دسته در نظر بگیرید: _جمع‌آوری داده‌ها_ و _طراحی الگوریتم_.

در جمع‌آوری داده‌ها، اقدامات احتمالاً شامل **داده‌های شخصی** یا اطلاعات شناسایی شخصی (PII) برای افراد قابل شناسایی خواهد بود. این شامل [موارد متنوعی از داده‌های غیرشخصی](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) است که _به طور جمعی_ یک فرد را شناسایی می‌کنند. چالش‌های اخلاقی می‌توانند به موضوعاتی مانند _حریم خصوصی داده‌ها_، _مالکیت داده‌ها_ و موضوعات مرتبط مانند _رضایت آگاهانه_ و _حقوق مالکیت فکری_ برای کاربران مربوط شوند.

در طراحی الگوریتم، اقدامات شامل جمع‌آوری و مدیریت **مجموعه داده‌ها** و سپس استفاده از آن‌ها برای آموزش و استقرار **مدل‌های داده** است که نتایج را پیش‌بینی یا تصمیمات را در زمینه‌های واقعی خودکار می‌کنند. چالش‌های اخلاقی می‌توانند از _تعصب در مجموعه داده‌ها_، مشکلات _کیفیت داده‌ها_، _بی‌عدالتی_ و _تحریف_ در الگوریتم‌ها ناشی شوند - از جمله برخی مسائل که ذاتاً سیستماتیک هستند.

در هر دو مورد، چالش‌های اخلاقی مناطقی را برجسته می‌کنند که اقدامات ما ممکن است با ارزش‌های مشترک ما در تضاد باشند. برای شناسایی، کاهش، به حداقل رساندن یا حذف این نگرانی‌ها، باید سوالات اخلاقی "بله/خیر" مرتبط با اقدامات خود را بپرسیم و سپس اقدامات اصلاحی لازم را انجام دهیم. بیایید به برخی از چالش‌های اخلاقی و سوالات اخلاقی که مطرح می‌کنند نگاهی بیندازیم:

#### 2.1 مالکیت داده‌ها

جمع‌آوری داده‌ها اغلب شامل داده‌های شخصی است که می‌تواند افراد را شناسایی کند. [مالکیت داده‌ها](https://permission.io/blog/data-ownership) درباره _کنترل_ و [_حقوق کاربران_](https://permission.io/blog/data-ownership) مرتبط با ایجاد، پردازش و انتشار داده‌ها است.

سوالات اخلاقی که باید بپرسیم:
* چه کسی مالک داده‌ها است؟ (کاربر یا سازمان)
* کاربران چه حقوقی دارند؟ (مثلاً دسترسی، حذف، قابلیت انتقال)
* سازمان‌ها چه حقوقی دارند؟ (مثلاً اصلاح نظرات مخرب کاربران)

#### 2.2 رضایت آگاهانه

[رضایت آگاهانه](https://legaldictionary.net/informed-consent/) به عمل موافقت کاربران با یک اقدام (مانند جمع‌آوری داده‌ها) با _درک کامل_ حقایق مرتبط از جمله هدف، خطرات احتمالی و جایگزین‌ها اشاره دارد.

سوالاتی که باید بررسی شوند:
* آیا کاربر (موضوع داده) اجازه جمع‌آوری و استفاده از داده‌ها را داده است؟
* آیا کاربر هدف از جمع‌آوری داده‌ها را درک کرده است؟
* آیا کاربر خطرات احتمالی مشارکت خود را درک کرده است؟

#### 2.3 مالکیت فکری

[مالکیت فکری](https://en.wikipedia.org/wiki/Intellectual_property) به خلاقیت‌های ناملموسی اشاره دارد که از ابتکار انسانی ناشی می‌شوند و ممکن است _ارزش اقتصادی_ برای افراد یا کسب‌وکارها داشته باشند.

سوالاتی که باید بررسی شوند:
* آیا داده‌های جمع‌آوری‌شده ارزش اقتصادی برای کاربر یا کسب‌وکار داشته‌اند؟
* آیا **کاربر** در اینجا مالکیت فکری دارد؟
* آیا **سازمان** در اینجا مالکیت فکری دارد؟
* اگر این حقوق وجود دارند، چگونه از آن‌ها محافظت می‌کنیم؟

#### 2.4 حریم خصوصی داده‌ها

[حریم خصوصی داده‌ها](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) یا حریم اطلاعات به حفظ حریم خصوصی کاربران و حفاظت از هویت آن‌ها در ارتباط با اطلاعات شناسایی شخصی اشاره دارد.

سوالاتی که باید بررسی شوند:
* آیا داده‌های (شخصی) کاربران در برابر هک و نشت محافظت شده‌اند؟
* آیا داده‌های کاربران فقط برای کاربران و زمینه‌های مجاز قابل دسترسی هستند؟
* آیا ناشناس بودن کاربران هنگام اشتراک‌گذاری یا انتشار داده‌ها حفظ شده است؟
* آیا می‌توان یک کاربر را از مجموعه داده‌های ناشناس شناسایی کرد؟

#### 2.5 حق فراموش شدن

[حق فراموش شدن](https://en.wikipedia.org/wiki/Right_to_be_forgotten) یا [حق حذف](https://www.gdpreu.org/right-to-be-forgotten/) حفاظت بیشتری از داده‌های شخصی به کاربران ارائه می‌دهد. به طور خاص، این حق به کاربران اجازه می‌دهد درخواست حذف یا حذف داده‌های شخصی خود را از جستجوهای اینترنتی و مکان‌های دیگر، _تحت شرایط خاص_، ارائه دهند - و به آن‌ها اجازه می‌دهد بدون اینکه اقدامات گذشته علیه آن‌ها استفاده شود، یک شروع تازه آنلاین داشته باشند.

سوالاتی که باید بررسی شوند:
* آیا سیستم اجازه می‌دهد که موضوعات داده درخواست حذف کنند؟
* آیا باید لغو رضایت کاربر باعث حذف خودکار شود؟
* آیا داده‌ها بدون رضایت یا به روش‌های غیرقانونی جمع‌آوری شده‌اند؟
* آیا ما با مقررات دولتی برای حریم خصوصی داده‌ها مطابقت داریم؟

#### 2.6 تعصب در مجموعه داده‌ها

تعصب در مجموعه داده‌ها یا [تعصب در جمع‌آوری](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) به انتخاب یک زیرمجموعه _غیرنماینده_ از داده‌ها برای توسعه الگوریتم اشاره دارد که ممکن است منجر به نتایج ناعادلانه برای گروه‌های مختلف شود. انواع تعصب شامل تعصب در انتخاب یا نمونه‌گیری، تعصب داوطلبانه و تعصب ابزاری است.

سوالاتی که باید بررسی شوند:
* آیا ما مجموعه‌ای نماینده از موضوعات داده را انتخاب کرده‌ایم؟
* آیا مجموعه داده‌های جمع‌آوری‌شده یا مدیریت‌شده خود را از نظر تعصبات مختلف آزمایش کرده‌ایم؟
* آیا می‌توانیم تعصبات کشف‌شده را کاهش یا حذف کنیم؟

#### 2.7 کیفیت داده‌ها

[کیفیت داده‌ها](https://lakefs.io/data-quality-testing/) به اعتبار مجموعه داده مدیریت‌شده برای توسعه الگوریتم‌های ما اشاره دارد و بررسی می‌کند که آیا ویژگی‌ها و رکوردها الزامات سطح دقت و سازگاری مورد نیاز برای هدف هوش مصنوعی ما را برآورده می‌کنند یا خیر.

سوالاتی که باید بررسی شوند:
* آیا ما ویژگی‌های معتبر برای مورد استفاده خود را جمع‌آوری کرده‌ایم؟
* آیا داده‌ها به طور _سازگار_ از منابع داده متنوع جمع‌آوری شده‌اند؟
* آیا مجموعه داده برای شرایط یا سناریوهای متنوع _کامل_ است؟
* آیا اطلاعات به طور _دقیق_ واقعیت را منعکس می‌کنند؟

#### 2.8 عدالت الگوریتمی
[بررسی عدالت الگوریتمی](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) به این موضوع می‌پردازد که آیا طراحی الگوریتم به طور سیستماتیک علیه گروه‌های خاصی از افراد تبعیض قائل می‌شود و منجر به [آسیب‌های احتمالی](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) در _تخصیص منابع_ (جایی که منابع از آن گروه محروم یا دریغ می‌شوند) و _کیفیت خدمات_ (جایی که دقت هوش مصنوعی برای برخی گروه‌ها کمتر از دیگران است) می‌شود.

سؤالاتی که باید در اینجا بررسی شوند:
* آیا دقت مدل را برای گروه‌ها و شرایط متنوع ارزیابی کرده‌ایم؟
* آیا سیستم را برای آسیب‌های احتمالی (مانند کلیشه‌سازی) بررسی کرده‌ایم؟
* آیا می‌توانیم داده‌ها را اصلاح کنیم یا مدل‌ها را دوباره آموزش دهیم تا آسیب‌های شناسایی‌شده را کاهش دهیم؟

منابعی مانند [چک‌لیست‌های عدالت هوش مصنوعی](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) را بررسی کنید تا اطلاعات بیشتری کسب کنید.

#### 2.9 تحریف داده‌ها

[تحریف داده‌ها](https://www.sciencedirect.com/topics/computer-science/misrepresentation) به این موضوع می‌پردازد که آیا ما بینش‌های حاصل از داده‌های گزارش‌شده را به صورت صادقانه اما به شکلی فریبنده برای حمایت از یک روایت مطلوب ارائه می‌دهیم.

سؤالاتی که باید در اینجا بررسی شوند:
* آیا داده‌های ناقص یا نادرست را گزارش می‌دهیم؟
* آیا داده‌ها را به شکلی تجسم می‌کنیم که منجر به نتیجه‌گیری‌های گمراه‌کننده شود؟
* آیا از تکنیک‌های آماری انتخابی برای دستکاری نتایج استفاده می‌کنیم؟
* آیا توضیحات جایگزینی وجود دارد که ممکن است نتیجه متفاوتی ارائه دهد؟

#### 2.10 انتخاب آزاد

[توهم انتخاب آزاد](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) زمانی رخ می‌دهد که "معماری انتخاب" سیستم از الگوریتم‌های تصمیم‌گیری برای هدایت افراد به سمت یک نتیجه ترجیحی استفاده می‌کند، در حالی که به نظر می‌رسد به آن‌ها گزینه‌ها و کنترل داده شده است. این [الگوهای تاریک](https://www.darkpatterns.org/) می‌توانند به کاربران آسیب‌های اجتماعی و اقتصادی وارد کنند. از آنجا که تصمیمات کاربران بر پروفایل‌های رفتاری تأثیر می‌گذارد، این اقدامات ممکن است انتخاب‌های آینده را تقویت یا گسترش دهد و تأثیر این آسیب‌ها را افزایش دهد.

سؤالاتی که باید در اینجا بررسی شوند:
* آیا کاربر پیامدهای انتخاب خود را درک کرده است؟
* آیا کاربر از گزینه‌های (جایگزین) و مزایا و معایب هر یک آگاه بوده است؟
* آیا کاربر می‌تواند یک انتخاب خودکار یا تحت تأثیر قرار گرفته را بعداً تغییر دهد؟

### 3. مطالعات موردی

برای قرار دادن این چالش‌های اخلاقی در زمینه‌های واقعی، بررسی مطالعات موردی که آسیب‌ها و پیامدهای احتمالی برای افراد و جامعه را نشان می‌دهند، زمانی که چنین نقض‌های اخلاقی نادیده گرفته می‌شوند، مفید است.

در اینجا چند مثال آورده شده است:

| چالش اخلاقی | مطالعه موردی | 
|--- |--- |
| **رضایت آگاهانه** | 1972 - [مطالعه سیفلیس توسکیگی](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - مردان آفریقایی-آمریکایی که در این مطالعه شرکت کردند وعده مراقبت پزشکی رایگان داده شد _اما توسط محققان فریب داده شدند_ که به آن‌ها در مورد تشخیص بیماری یا در دسترس بودن درمان اطلاع ندادند. بسیاری از شرکت‌کنندگان جان خود را از دست دادند و شرکا یا فرزندان آن‌ها تحت تأثیر قرار گرفتند؛ این مطالعه 40 سال طول کشید. | 
| **حریم خصوصی داده‌ها** | 2007 - [جایزه داده‌های نتفلیکس](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) به محققان _10 میلیون رتبه‌بندی فیلم ناشناس از 50 هزار مشتری_ ارائه شد تا به بهبود الگوریتم‌های توصیه کمک کند. با این حال، محققان توانستند داده‌های ناشناس را با داده‌های قابل شناسایی شخصی در _مجموعه‌های داده خارجی_ (مانند نظرات IMDb) مرتبط کنند - به طور مؤثر برخی از مشترکان نتفلیکس را "غیرناشناس" کردند.|
| **تعصب در جمع‌آوری داده‌ها** | 2013 - شهر بوستون [اپلیکیشن Street Bump](https://www.boston.gov/transportation/street-bump) را توسعه داد که به شهروندان اجازه می‌داد چاله‌ها را گزارش دهند و به شهر داده‌های بهتر جاده‌ای برای یافتن و رفع مشکلات ارائه دهد. با این حال، [افراد در گروه‌های کم‌درآمد دسترسی کمتری به خودروها و تلفن‌ها داشتند](https://hbr.org/2013/04/the-hidden-biases-in-big-data)، که مشکلات جاده‌ای آن‌ها را در این اپلیکیشن نامرئی می‌کرد. توسعه‌دهندگان با دانشگاهیان همکاری کردند تا مسائل مربوط به _دسترسی عادلانه و شکاف‌های دیجیتال_ را برای عدالت حل کنند. |
| **عدالت الگوریتمی** | 2018 - مطالعه [Gender Shades](http://gendershades.org/overview.html) MIT دقت محصولات هوش مصنوعی در طبقه‌بندی جنسیت را ارزیابی کرد و شکاف‌های دقت برای زنان و افراد رنگین‌پوست را آشکار کرد. یک [کارت اپل 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) به نظر می‌رسید که اعتبار کمتری به زنان نسبت به مردان ارائه می‌دهد. هر دو مسائل مربوط به تعصب الگوریتمی را نشان دادند که منجر به آسیب‌های اجتماعی-اقتصادی شد.|
| **تحریف داده‌ها** | 2020 - [وزارت بهداشت جورجیا نمودارهای COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) را منتشر کرد که به نظر می‌رسید شهروندان را در مورد روند موارد تأییدشده با ترتیب غیرزمانی در محور x گمراه کند. این مثال تحریف از طریق ترفندهای تجسم را نشان می‌دهد. |
| **توهم انتخاب آزاد** | 2020 - اپلیکیشن یادگیری [ABCmouse مبلغ 10 میلیون دلار برای حل شکایت FTC پرداخت کرد](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) که والدین را مجبور به پرداخت اشتراک‌هایی کرد که نمی‌توانستند لغو کنند. این مثال الگوهای تاریک در معماری انتخاب را نشان می‌دهد، جایی که کاربران به سمت انتخاب‌های بالقوه مضر هدایت شدند. |
| **حریم خصوصی داده‌ها و حقوق کاربران** | 2021 - [نشت داده‌های فیسبوک](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) داده‌های 530 میلیون کاربر را افشا کرد و منجر به تسویه 5 میلیارد دلاری با FTC شد. با این حال، فیسبوک از اطلاع‌رسانی به کاربران در مورد این نشت خودداری کرد و حقوق کاربران در مورد شفافیت داده‌ها و دسترسی را نقض کرد. |

آیا می‌خواهید مطالعات موردی بیشتری را بررسی کنید؟ این منابع را بررسی کنید:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - معضلات اخلاقی در صنایع مختلف.
* [دوره اخلاق در علم داده](https://www.coursera.org/learn/data-science-ethics#syllabus) - مطالعات موردی برجسته بررسی شده.
* [جایی که اشتباه رخ داده است](https://deon.drivendata.org/examples/) - چک‌لیست Deon با مثال‌ها.

> 🚨 به مطالعات موردی که دیده‌اید فکر کنید - آیا شما تجربه یا تحت تأثیر چالش اخلاقی مشابهی در زندگی خود قرار گرفته‌اید؟ آیا می‌توانید حداقل یک مطالعه موردی دیگر را که یکی از چالش‌های اخلاقی مورد بحث در این بخش را نشان می‌دهد، به یاد بیاورید؟

## اخلاق کاربردی

ما درباره مفاهیم اخلاقی، چالش‌ها و مطالعات موردی در زمینه‌های واقعی صحبت کردیم. اما چگونه می‌توانیم اصول و شیوه‌های اخلاقی را در پروژه‌های خود _اعمال_ کنیم؟ و چگونه می‌توانیم این شیوه‌ها را برای حکمرانی بهتر _عملیاتی_ کنیم؟ بیایید برخی از راه‌حل‌های واقعی را بررسی کنیم:

### 1. کدهای حرفه‌ای

کدهای حرفه‌ای یک گزینه برای سازمان‌ها ارائه می‌دهند تا اعضا را "تشویق" کنند که از اصول اخلاقی و بیانیه مأموریت آن‌ها حمایت کنند. کدها _راهنمای اخلاقی_ برای رفتار حرفه‌ای هستند و به کارکنان یا اعضا کمک می‌کنند تصمیماتی بگیرند که با اصول سازمانشان همسو باشد. آن‌ها تنها به اندازه پایبندی داوطلبانه اعضا مؤثر هستند؛ با این حال، بسیاری از سازمان‌ها پاداش‌ها و مجازات‌های اضافی برای انگیزه دادن به اعضا برای پایبندی ارائه می‌دهند.

نمونه‌ها شامل موارد زیر هستند:

* [کد اخلاق آکسفورد مونیخ](http://www.code-of-ethics.org/code-of-conduct/)
* [کد رفتار انجمن علم داده](http://datascienceassn.org/code-of-conduct.html) (ایجاد شده در 2013)
* [کد اخلاق و رفتار حرفه‌ای ACM](https://www.acm.org/code-of-ethics) (از سال 1993)

> 🚨 آیا شما عضو یک سازمان حرفه‌ای مهندسی یا علم داده هستید؟ سایت آن‌ها را بررسی کنید تا ببینید آیا کد اخلاق حرفه‌ای تعریف کرده‌اند. این کد درباره اصول اخلاقی آن‌ها چه می‌گوید؟ چگونه اعضا را "تشویق" می‌کنند که از کد پیروی کنند؟

### 2. چک‌لیست‌های اخلاقی

در حالی که کدهای حرفه‌ای رفتار اخلاقی مورد نیاز را از متخصصان تعریف می‌کنند، آن‌ها [محدودیت‌های شناخته‌شده‌ای](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) در اجرا دارند، به ویژه در پروژه‌های بزرگ‌مقیاس. در عوض، بسیاری از متخصصان علم داده [از چک‌لیست‌ها حمایت می‌کنند](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) که می‌توانند **اصول را به شیوه‌ها** به صورت قطعی‌تر و قابل اجرا تبدیل کنند.

چک‌لیست‌ها سؤالات را به وظایف "بله/خیر" تبدیل می‌کنند که می‌توانند عملیاتی شوند و به عنوان بخشی از جریان‌های کاری استاندارد انتشار محصول پیگیری شوند.

نمونه‌ها شامل موارد زیر هستند:
* [Deon](https://deon.drivendata.org/) - یک چک‌لیست اخلاق داده عمومی که از [توصیه‌های صنعتی](https://deon.drivendata.org/#checklist-citations) ایجاد شده و دارای ابزار خط فرمان برای یکپارچه‌سازی آسان است.
* [چک‌لیست ممیزی حریم خصوصی](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - راهنمایی کلی برای شیوه‌های مدیریت اطلاعات از دیدگاه‌های قانونی و اجتماعی ارائه می‌دهد.
* [چک‌لیست عدالت هوش مصنوعی](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - توسط متخصصان هوش مصنوعی ایجاد شده تا از پذیرش و یکپارچه‌سازی بررسی‌های عدالت در چرخه‌های توسعه هوش مصنوعی حمایت کند.
* [22 سؤال برای اخلاق در داده‌ها و هوش مصنوعی](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - چارچوبی بازتر، ساختار یافته برای بررسی اولیه مسائل اخلاقی در طراحی، اجرا و زمینه‌های سازمانی.

### 3. مقررات اخلاقی

اخلاق درباره تعریف ارزش‌های مشترک و انجام کار درست به صورت _داوطلبانه_ است. **پایبندی** درباره _پیروی از قانون_ در صورت تعریف شدن است. **حکمرانی** به طور کلی شامل تمام روش‌هایی است که سازمان‌ها برای اجرای اصول اخلاقی و پایبندی به قوانین تعریف‌شده عمل می‌کنند.

امروزه، حکمرانی در سازمان‌ها دو شکل دارد. اول، تعریف اصول **هوش مصنوعی اخلاقی** و ایجاد شیوه‌هایی برای عملیاتی کردن پذیرش در تمام پروژه‌های مرتبط با هوش مصنوعی در سازمان. دوم، پایبندی به تمام مقررات حفاظت از داده‌های دولتی برای مناطقی که در آن فعالیت می‌کنند.

نمونه‌هایی از مقررات حفاظت از داده‌ها و حریم خصوصی:

* `1974`، [قانون حریم خصوصی ایالات متحده](https://www.justice.gov/opcl/privacy-act-1974) - جمع‌آوری، استفاده و افشای اطلاعات شخصی توسط _دولت فدرال_ را تنظیم می‌کند.
* `1996`، [قانون قابلیت حمل و حفاظت از بیمه سلامت ایالات متحده (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - از داده‌های سلامت شخصی محافظت می‌کند.
* `1998`، [قانون حفاظت از حریم خصوصی آنلاین کودکان ایالات متحده (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - از حریم خصوصی داده‌های کودکان زیر 13 سال محافظت می‌کند.
* `2018`، [مقررات عمومی حفاظت از داده‌ها (GDPR)](https://gdpr-info.eu/) - حقوق کاربران، حفاظت از داده‌ها و حریم خصوصی را فراهم می‌کند.
* `2018`، [قانون حریم خصوصی مصرف‌کننده کالیفرنیا (CCPA)](https://www.oag.ca.gov/privacy/ccpa) به مصرف‌کنندگان حقوق بیشتری بر داده‌های (شخصی) خود می‌دهد.
* `2021`، [قانون حفاظت از اطلاعات شخصی چین](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) که به تازگی تصویب شده و یکی از قوی‌ترین مقررات حریم خصوصی داده‌های آنلاین در جهان را ایجاد کرده است.

> 🚨 اتحادیه اروپا GDPR (مقررات عمومی حفاظت از داده‌ها) را تعریف کرده است که یکی از تأثیرگذارترین مقررات حریم خصوصی داده‌ها امروز است. آیا می‌دانستید که این مقررات همچنین [8 حق کاربر](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) را برای حفاظت از حریم خصوصی دیجیتال و داده‌های شخصی شهروندان تعریف می‌کند؟ درباره این حقوق و اهمیت آن‌ها اطلاعات کسب کنید.

### 4. فرهنگ اخلاقی

توجه داشته باشید که شکاف ناملموسی بین _پایبندی_ (انجام کافی برای رعایت "متن قانون") و پرداختن به [مسائل سیستماتیک](https://www.coursera.org/learn/data-science-ethics/home/week/4) (مانند سخت‌شدگی، عدم تقارن اطلاعات و نابرابری توزیعی) وجود دارد که می‌تواند تسریع در تسلیح هوش مصنوعی را سرعت بخشد.

دومی نیاز به [رویکردهای همکاری برای تعریف فرهنگ‌های اخلاقی](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) دارد که ارتباطات احساسی و ارزش‌های مشترک سازگار را _در سراسر سازمان‌ها_ در صنعت ایجاد می‌کند. این نیاز به [فرهنگ‌های اخلاقی داده رسمی‌تر](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) در سازمان‌ها دارد - اجازه دادن به _هر کسی_ برای [کشیدن طناب Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (برای مطرح کردن نگرانی‌های اخلاقی در اوایل فرآیند) و قرار دادن _ارزیابی‌های اخلاقی_ (مانند در استخدام) به عنوان معیار اصلی تشکیل تیم در پروژه‌های هوش مصنوعی.

---
## [آزمون پس از سخنرانی](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/3) 🎯
## مرور و مطالعه شخصی

دوره‌ها و کتاب‌ها به درک مفاهیم اصلی اخلاق و چالش‌ها کمک می‌کنند، در حالی که مطالعات موردی و ابزارها به شیوه‌های اخلاقی کاربردی در زمینه‌های واقعی کمک می‌کنند. در اینجا چند منبع برای شروع آورده شده است:

* [یادگیری ماشین برای مبتدیان](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - درس عدالت، از مایکروسافت.
* [اصول هوش مصنوعی مسئولانه](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - مسیر یادگیری رایگان از Microsoft Learn.  
* [اخلاق و علم داده](https://resources.oreilly.com/examples/0636920203964) - کتاب الکترونیکی از O'Reilly (نوشته M. Loukides, H. Mason و دیگران)  
* [اخلاق در علم داده](https://www.coursera.org/learn/data-science-ethics#syllabus) - دوره آنلاین از دانشگاه میشیگان.  
* [اخلاق بازگشایی‌شده](https://ethicsunwrapped.utexas.edu/case-studies) - مطالعات موردی از دانشگاه تگزاس.  

# تکلیف  

[نوشتن یک مطالعه موردی در اخلاق داده](assignment.md)  

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.